[package]
name = "flue-flash-attn-v2"
version = "0.8.0"
edition = "2021"

description = "Flash attention V2 layer for Flue!"
keywords = ["blas", "tensor", "machine-learning"]
categories = ["science"]
license = "MIT OR Apache-2.0"
readme = "README.md"
repository = "https://github.com/Apsu/flue"

[dependencies]
# TODO: Uncomment when candle 0.9.0-alpha.2 is pushed to crates.io
# candle-core = { version = "0.9.0-alpha.2", features = ["cuda"] }
candle-core = { git = "https://github.com/huggingface/candle", branch = "main", features = ["cuda"] }
half = { version = "2.3.1", features = ["num-traits"] }

[build-dependencies]
bindgen_cuda = "0.1.1"
anyhow = { version = "1", features = ["backtrace"] }


[dev-dependencies]
anyhow = { version = "1", features = ["backtrace"] }
# TODO: Uncomment when candle 0.9.0-alpha.2 is pushed to crates.io
# candle-nn = { version = "0.9.0-alpha.2", features = ["cuda"] }
candle-nn = { git = "https://github.com/huggingface/candle", branch = "main", features = ["cuda"] }
